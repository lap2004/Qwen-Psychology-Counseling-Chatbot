import streamlit as st
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel, PeftConfig
import torch

# T√™n th∆∞ m·ª•c ch·ª©a adapter model
ADAPTER_PATH = "./lap_qwen-mental-health-finetuned"

# Load model & tokenizer
@st.cache_resource
def load_model():
    # Load th√¥ng tin adapter
    config = PeftConfig.from_pretrained(ADAPTER_PATH)

    # Load base model (t·ª´ HuggingFace ho·∫∑c local)
    base_model = AutoModelForCausalLM.from_pretrained(
        config.base_model_name_or_path,
        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
        device_map="auto"
    )

    # √Åp d·ª•ng adapter v√†o base model
    model = PeftModel.from_pretrained(base_model, ADAPTER_PATH)
    tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)

    model.eval()
    return tokenizer, model

# Giao di·ªán Streamlit
st.set_page_config(page_title="Chatbot T√¢m l√Ω (Adapter)", layout="centered")
st.title("üß† Chatbot T∆∞ v·∫•n T√¢m l√Ω")

with st.spinner("üîß ƒêang t·∫£i m√¥ h√¨nh..."):
    tokenizer, model = load_model()

user_input = st.text_area("üí¨ Nh·∫≠p c√¢u h·ªèi c·ªßa b·∫°n:", height=100)

if st.button("üß† Tr·∫£ l·ªùi"):
    if not user_input.strip():
        st.warning("‚õî Vui l√≤ng nh·∫≠p c√¢u h·ªèi.")
    else:
        with st.spinner("ü§ñ ƒêang sinh ph·∫£n h·ªìi..."):
            prompt = f"<|user|>\n{user_input.strip()}\n<|assistant|>\n"
            inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

            with torch.no_grad():
                output = model.generate(
                    **inputs,
                    max_new_tokens=512,
                    temperature=0.7,
                    top_p=0.9,
                    do_sample=True,
                    pad_token_id=tokenizer.eos_token_id
                )

            decoded = tokenizer.decode(output[0], skip_special_tokens=True)
            response = decoded.split("<|assistant|>")[-1].strip()
            st.success(response)
